{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is a starter notebook to get you familiar with how to browse the data and train a simple neural network using Tensorflow/Keras.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport os\nimport pandas as pd \n\nfrom PIL import Image\n\nimport keras\nimport keras.layers\nfrom keras.models import Sequential \nfrom keras.layers import Dense, Dropout, Flatten \nfrom keras.layers import Conv2D, MaxPooling2D, InputLayer\nimport keras.utils.all_utils as kr_utils\nimport keras.regularizers\nfrom keras.preprocessing.image import ImageDataGenerator\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\nfrom tensorflow.keras.layers import BatchNormalization\nfrom sklearn.model_selection import train_test_split\n\nprint(tf.__version__)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-04T06:43:07.118003Z","iopub.execute_input":"2022-04-04T06:43:07.118315Z","iopub.status.idle":"2022-04-04T06:43:13.392260Z","shell.execute_reply.started":"2022-04-04T06:43:07.118235Z","shell.execute_reply":"2022-04-04T06:43:13.390775Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train_images_folder = \"/kaggle/input/csci-5622-spring-22/train/train/\"\ntest_images_folder = \"/kaggle/input/csci-5622-spring-22/test/test\"\ntrain_csv = \"/kaggle/input/csci-5622-spring-22/train.csv\"\nsubmission_csv = \"/kaggle/input/csci-5622-spring-22/sample_submission.csv\"\npatch_size = 192\nnum_classes = 53","metadata":{"execution":{"iopub.status.busy":"2022-04-04T06:43:13.393875Z","iopub.execute_input":"2022-04-04T06:43:13.394098Z","iopub.status.idle":"2022-04-04T06:43:13.401174Z","shell.execute_reply.started":"2022-04-04T06:43:13.394071Z","shell.execute_reply":"2022-04-04T06:43:13.400486Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"First, let's create a generator that will read the images and provide batches of samples and their corresponding labels.\n\nWe'll be building it using keras' Sequence","metadata":{}},{"cell_type":"code","source":"class RockGenerator(kr_utils.Sequence):\n    def __init__(self, df, # contains the images names and their labels\n                 path_to_images,\n                 batch_size=32,\n                 shuffle=True, # to shuffle the data at the end of each epoch\n                ):\n        \n        self.df = df # dataframe with two columns \"image\" and \"label\"\n        self.images_path = path_to_images\n        self.batch_size = batch_size\n        self.shuffle = shuffle\n        \n        self.mean = 125.3\n        self.std = 63.5\n        if shuffle:\n            self.indexes = np.random.permutation(self.df.shape[0])\n        else:\n            self.indexes = np.arange(self.df.shape[0])\n        self.on_epoch_end()\n\n    def on_epoch_end(self): # called at the end of each epoch\n        if self.shuffle:\n            np.random.shuffle(self.indexes)\n\n    def __len__(self):\n        # return number of batches in dataset / steps per epoch\n        return int(np.ceil(self.df.shape[0] / self.batch_size))\n\n    def __getitem__(self, index):\n        # get batch at position index\n        indexes = self.df.index[self.indexes[index*self.batch_size:min((index+1)*self.batch_size, self.df.shape[0])] ] \n        images = np.zeros((len(indexes), patch_size, patch_size,3))\n        labels = np.zeros((len(indexes), num_classes))\n        for i, ind in enumerate(indexes):\n            image = np.asarray(Image.open(os.path.join(self.images_path , \"{}.png\".format(self.df.image[ind]))))\n            image = (image - self.mean) / self.std # this is global mean and std, you can use mean/std per channel\n            images[i] = image\n            labels[i] = kr_utils.to_categorical(self.df.label[ind], num_classes=num_classes) # gives the one-hot-encoding\n        return images, labels","metadata":{"execution":{"iopub.status.busy":"2022-04-04T06:43:13.402489Z","iopub.execute_input":"2022-04-04T06:43:13.402920Z","iopub.status.idle":"2022-04-04T06:43:13.431148Z","shell.execute_reply.started":"2022-04-04T06:43:13.402881Z","shell.execute_reply":"2022-04-04T06:43:13.430278Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df_train, df_val = train_test_split(pd.read_csv(train_csv), test_size = 0.1, random_state = 5622)\ndf_test = pd.read_csv(submission_csv)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T06:43:13.435209Z","iopub.execute_input":"2022-04-04T06:43:13.435458Z","iopub.status.idle":"2022-04-04T06:43:13.477867Z","shell.execute_reply.started":"2022-04-04T06:43:13.435410Z","shell.execute_reply":"2022-04-04T06:43:13.477127Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"print(df_train.shape, df_val.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T06:43:13.479226Z","iopub.execute_input":"2022-04-04T06:43:13.479511Z","iopub.status.idle":"2022-04-04T06:43:13.485308Z","shell.execute_reply.started":"2022-04-04T06:43:13.479473Z","shell.execute_reply":"2022-04-04T06:43:13.484574Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_generator = RockGenerator(df_train, train_images_folder)\nval_generator = RockGenerator(df_val, train_images_folder, shuffle=False)\ntest_generator = RockGenerator(df_test, test_images_folder, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T06:43:13.486968Z","iopub.execute_input":"2022-04-04T06:43:13.487478Z","iopub.status.idle":"2022-04-04T06:43:13.495347Z","shell.execute_reply.started":"2022-04-04T06:43:13.487436Z","shell.execute_reply":"2022-04-04T06:43:13.494291Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# let's examine a batch\nbatch_x, batch_y = train_generator[0]\nprint(batch_x.shape, batch_y.shape)\nprint(np.mean(batch_x), np.std(batch_x)) # not exactly 0 and 1, but close enough","metadata":{"execution":{"iopub.status.busy":"2022-04-04T06:43:13.497004Z","iopub.execute_input":"2022-04-04T06:43:13.497307Z","iopub.status.idle":"2022-04-04T06:43:13.752666Z","shell.execute_reply.started":"2022-04-04T06:43:13.497233Z","shell.execute_reply":"2022-04-04T06:43:13.751814Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Let's now define a simple neural network. We'll start with a stack of 2d convolutions followed by a few feed-forward network.","metadata":{}},{"cell_type":"code","source":"# model = keras.models.Sequential([keras.layers.InputLayer(input_shape=(patch_size,patch_size,3)), # Input layer, no need to mention the batch_size\n#                                  keras.layers.Conv2D(filters=32, kernel_size=(4,4), strides=2,activation=\"relu\"),\n#                                  #keras.layers.add(BatchNormalization()),\n#                                  keras.layers.MaxPooling2D(2,2),\n#                                  keras.layers.Conv2D(filters=32, kernel_size=(4,4), strides=2,activation=\"relu\"),\n#                                  #keras.layers.add(BatchNormalization()),\n#                                  keras.layers.MaxPooling2D(2,2),\n#                                  keras.layers.Conv2D(filters=64, kernel_size=(3,3), strides=2,activation=\"relu\"),\n#                                  keras.layers.add(BatchNormalization()),\n#                                  keras.layers.MaxPooling2D(2,2),\n#                                  keras.layers.Flatten(),\n#                                  keras.layers.Dense(128, activation=\"relu\"),\n#                                  keras.layers.Dense(128, activation=\"relu\"),\n#                                  keras.layers.Dense(53, activation=\"softmax\")\n#                                  ])\nmodel = keras.models.Sequential()\nmodel.add(InputLayer(input_shape=(patch_size,patch_size,3)))\nmodel.add(Conv2D(filters=32, kernel_size=(4,4), strides=2,activation=\"relu\")) \nmodel.add(MaxPooling2D(2,2))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters=32, kernel_size=(4,4), strides=2,activation=\"relu\")) \nmodel.add(MaxPooling2D(2,2))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters=64, kernel_size=(3,3), strides=2,activation=\"relu\")) \nmodel.add(MaxPooling2D(2,2))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\n#model.add(Dense(units=128,activation = 'relu'))\n#model.add(Dropout(0.25))\nmodel.add(Dense(units = 128, activation = 'relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(units = 128, activation = 'relu'))\nmodel.add(Dense(units = 53, activation = 'softmax'))\nmodel.summary() # prints the output of each layer and number of trainable weights for each","metadata":{"execution":{"iopub.status.busy":"2022-04-04T06:43:17.936917Z","iopub.execute_input":"2022-04-04T06:43:17.937179Z","iopub.status.idle":"2022-04-04T06:43:20.484715Z","shell.execute_reply.started":"2022-04-04T06:43:17.937148Z","shell.execute_reply":"2022-04-04T06:43:20.483971Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"All we have to do at this step is to choose the proper loss function, optimizer and metric.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), # this means that teh network returns the log probabilities and not probas\n              optimizer=keras.optimizers.adam_v2.Adam(learning_rate=4e-4), # The optimizer that smooths the gradient\n              metrics=[\"accuracy\", \n                       tfa.metrics.F1Score(num_classes=num_classes,average=\"macro\", name=\"macroF1\")]) # We want to track accuracy and MacroF1","metadata":{"execution":{"iopub.status.busy":"2022-04-04T06:43:20.486675Z","iopub.execute_input":"2022-04-04T06:43:20.486936Z","iopub.status.idle":"2022-04-04T06:43:20.506059Z","shell.execute_reply.started":"2022-04-04T06:43:20.486891Z","shell.execute_reply":"2022-04-04T06:43:20.505420Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"We would also like to store the model from the epoch with the best validation metric. We can easily achieve so using \"Callbacks\"","metadata":{}},{"cell_type":"code","source":"checkpoint_callbk = tf.keras.callbacks.ModelCheckpoint(\n    \"best_tiny_model\", # name of file to save the best model to\n    monitor=\"val_macroF1\", # prefix val to specify that we want the model with best macroF1 on the validation data\n    verbose=1, # prints out when the model achieve a better epoch\n    mode=\"max\", # the monitored metric should be maximized\n    save_freq=\"epoch\", # clear\n    save_best_only=True, # of course, if not, every time a new best is achieved will be savedf differently\n    save_weights_only=True # this means that we don't have to save the architecture, if you change the architecture, you'll loose the old weights\n)\n","metadata":{"execution":{"iopub.status.busy":"2022-04-04T06:43:20.551396Z","iopub.execute_input":"2022-04-04T06:43:20.551864Z","iopub.status.idle":"2022-04-04T06:43:20.557004Z","shell.execute_reply.started":"2022-04-04T06:43:20.551831Z","shell.execute_reply":"2022-04-04T06:43:20.556080Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"We're all set now to run our tiny neural network. Training usign CPU is going to be slow the more convolutions we use.\nFeed-forward layers are much faster on CPu compared to convolutions.","metadata":{}},{"cell_type":"code","source":"model.fit(train_generator,callbacks=[checkpoint_callbk], epochs=100, validation_data=val_generator)","metadata":{"execution":{"iopub.status.busy":"2022-04-04T06:43:21.912884Z","iopub.execute_input":"2022-04-04T06:43:21.913678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Once the training is done, we can load the weights from the epoch with the best validation metric. If you run it for more epochs, you'll notice that it overfits quickly and reaches around 0.91 F1 on the training vs ~ 0.25 F1 on the validation.\n\nThe partitions are fairly similar, so the estimates from cross validation can be reliable.","metadata":{}},{"cell_type":"code","source":"model.load_weights(\"best_tiny_model\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_hat = model.predict(test_generator) # logits of the 53 classes\ny_hat = np.argmax(y_hat, axis=1) # take the classe with the hgiher logit\ntest_generator.df.label = y_hat\ntest_generator.df.to_csv(\"start_here_submission.csv\", index=False) # we don't want to add the column of indices","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}